{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "olive-repeat",
   "metadata": {},
   "source": [
    "<img src = 'fotos/logo_dani.jpeg'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "objective-volunteer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX \n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bulgarian-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(real, predicted):\n",
    "    return np.sqrt(((real - predicted) ** 2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-antique",
   "metadata": {},
   "source": [
    "## Ficheros y rutas de entrada/salida "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "following-chamber",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_in = '../../datos/datos_desarrollo'\n",
    "file1_in = 'consumo_final.csv'\n",
    "dir_out = dir_in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-ancient",
   "metadata": {},
   "source": [
    "## Primera estrategia de modelaje "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-section",
   "metadata": {},
   "source": [
    "### Carga de datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "clinical-aspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consumo = pd.read_csv(os.path.join(dir_in, file1_in), sep = ';')\n",
    "df_consumo.columns = [columna.lower() for columna in df_consumo.columns]\n",
    "df_consumo.rename(columns = {'fecha_inicio': 'mes_inicio_temp', 'fecha_fin': 'mes_fin_temp'}, inplace = True)\n",
    "df_consumo.date = pd.to_datetime(df_consumo.date, format = '%Y-%m-%d')\n",
    "precio_model = df_consumo[['ccaa', 'producto', 'volumen_miles_de_kg', 'valor_miles_de_€', 'precio_medio_kg', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "refined-framing",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmpl_model_dict = {product: {comunidad: precio_model[(precio_model.producto.eq(product))&\n",
    "                                                      (precio_model.ccaa.eq(comunidad))].drop(['producto', 'ccaa'], axis = 1)\n",
    "                              for comunidad in precio_model.ccaa.unique()} for product in precio_model.producto.unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "matched-immigration",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prueba = df_consumo[(df_consumo.ccaa.isin(['Andalucia', 'Aragon'])) & (df_consumo.producto.isin(['Patatas', 'Mango']))]\n",
    "prueba_model = df_prueba[['ccaa', 'producto', 'volumen_miles_de_kg', 'valor_miles_de_€', 'precio_medio_kg', 'date']]\n",
    "sample_model_dict = {product: {comunidad: prueba_model[(prueba_model.producto.eq(product))&\n",
    "                                                      (prueba_model.ccaa.eq(comunidad))].drop(['producto', 'ccaa'], axis = 1)\n",
    "                              for comunidad in prueba_model.ccaa.unique()} for product in prueba_model.producto.unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-classroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = d = q = range(0, 3)\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-conditions",
   "metadata": {},
   "source": [
    "### Modelaje "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-glossary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_sarima(variable, model_dict):\n",
    "    min_error_df = pd.DataFrame(columns = ['comunidad', 'producto', 'error'])\n",
    "    try:\n",
    "        len(predicciones)\n",
    "    except NameError:\n",
    "        predicciones = pd.DataFrame(columns = ['ccaa', 'producto'])\n",
    "    vuelta_general = 1\n",
    "    fila_df = 0\n",
    "    for producto, v in model_dict.items():\n",
    "        n_comunidad = 1\n",
    "        for comunidad in v.keys():\n",
    "            combinacion = 1\n",
    "            error_dict = {}\n",
    "            timeseries_data = model_dict[producto][comunidad][[variable, 'date']].copy()\n",
    "            timeseries_data.index = pd.DatetimeIndex(timeseries_data.date)\n",
    "            timeseries_data.drop('date', axis = 1, inplace = True)\n",
    "            timeseries_data.sort_index(inplace = True)\n",
    "\n",
    "            train = timeseries_data.loc[timeseries_data.index <= '2020-02-01']\n",
    "            test = timeseries_data.loc[timeseries_data.index > '2020-02-01']\n",
    "            \n",
    "            for param in pdq:\n",
    "                for seasonal_param in seasonal_pdq:\n",
    "                    print('Tienes', len(model_dict), 'productos y vas por el', vuelta_general, '. Este producto tiene', \n",
    "                          len(v), 'comunidades y vas por la', n_comunidad, 'Hay un total de', len(pdq) * len(seasonal_pdq), \n",
    "                          'combinaciones y vas por la', combinacion, '. La variable es', variable)\n",
    "                    try:\n",
    "                        print(param, seasonal_param)\n",
    "                        mod = SARIMAX(train, \n",
    "                            order = param,\n",
    "                            seasonal_order = seasonal_param, \n",
    "                            enforce_stationarity = False,\n",
    "                            enforce_invertibility = False)                \n",
    "                        result = mod.fit()\n",
    "                        ypred = result.get_forecast(steps = len(test)).predicted_mean.values\n",
    "                        error = rmse(test.values, ypred)\n",
    "                        error_dict.update({error: [param, seasonal_param]})\n",
    "                    except IndexError:\n",
    "                        continue\n",
    "                    combinacion += 1\n",
    "                    clear_output(wait = True)\n",
    "                    \n",
    "            min_error = min(list(error_dict.keys()))\n",
    "            min_error_df.loc[fila_df, ['comunidad', 'producto', 'error']] = comunidad, producto, min_error\n",
    "            final_model = SARIMAX(train, order = error_dict[min_error][0], seasonal_order = error_dict[min_error][1], \n",
    "                                  enforce_stationarity = False, enforce_invertibilty = False)\n",
    "            final_result = final_model.fit()\n",
    "            final_ypred = final_result.get_forecast(steps = len(test))\n",
    "            ypred_df = pd.DataFrame(final_ypred.predicted_mean).rename(columns = {'predicted_mean': str(variable) + '_predicted'})\n",
    "            pred_ci = final_ypred.conf_int()\n",
    "            pred_ci_df = pd.concat([ypred_df, pred_ci], axis = 1)\n",
    "            pred_ci_df[['ccaa', 'producto']] = comunidad, producto\n",
    "            \n",
    "            line = pd.to_datetime('2020-02-01', format = '%Y-%m-%d')\n",
    "            real_value = train.loc[line].values[0]\n",
    "            new_row =  pd.DataFrame({'ccaa': comunidad, 'producto': producto, (variable + '_predicted'): real_value, \n",
    "                         ('lower ' + variable):real_value, ('upper ' + variable): real_value}, \n",
    "                                    index = [line])\n",
    "            predicciones = pd.concat([predicciones, new_row, pred_ci_df], axis = 0)\n",
    "            n_comunidad += 1\n",
    "            fila_df += 1\n",
    "        vuelta_general += 1\n",
    "    return predicciones, min_error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-familiar",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_peque, error_peque = best_sarima('volumen_miles_de_kg', cmpl_model_dict)\n",
    "\n",
    "with open('df_peque.pkl', 'wb') as fp:\n",
    "    pickle.dump(df_peque, fp)    \n",
    "with open('error_peque.pkl', 'wb') as fp:\n",
    "    pickle.dump(error_peque, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-botswana",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('df_peque.pkl', 'rb') as fp:\n",
    "    df_peque = pickle.load(fp)\n",
    "with open('error_peque.pkl', 'rb') as fp:\n",
    "    error_peque = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-reference",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_peque2, error_peque2 = best_sarima('valor_miles_de_€', cmpl_model_dict)\n",
    "df_medi = pd.concat([df_peque, df_peque2], axis = 1)\n",
    "error_medi = pd.concat([error_peque, error_peque2], axis = 1)\n",
    "with open('df_medi.pkl', 'wb') as fp:\n",
    "    pickle.dump(df_medi, fp)    \n",
    "with open('error_medi.pkl', 'wb') as fp:\n",
    "    pickle.dump(error_medi, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-breakfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('df_medi.pkl', 'rb') as fp:\n",
    "    df_medi = pickle.load(fp)\n",
    "with open('error_medi.pkl', 'rb') as fp:\n",
    "    error_medi = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-sunset",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_peque3, error_peque3 = best_sarima('precio_medio_kg', cmpl_model_dict)\n",
    "df_gran = pd.concat([df_medi, df_peque3], axis = 1)\n",
    "error_gran = pd.concat([error_medi, error_peque3], axis = 1)\n",
    "with open('df_gran.pkl', 'wb') as fp:\n",
    "    pickle.dump(df_gran, fp)    \n",
    "with open('error_gran.pkl', 'wb') as fp:\n",
    "    pickle.dump(error_gran, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-graphic",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('df_gran.pkl', 'rb') as fp:\n",
    "    df_gran = pickle.load(fp)\n",
    "with open('error_gran.pkl', 'rb') as fp:\n",
    "    error_gran = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-trust",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_gran.T.drop_duplicates().T\n",
    "df_final = df_final.loc[:, 'producto':]\n",
    "df_final.to_csv(os.path.join(dir_out, 'predicciones_consumo.csv'), sep = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-imperial",
   "metadata": {},
   "source": [
    "## Segunda estrategia de modelaje "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-monday",
   "metadata": {},
   "source": [
    "### Preparación de los datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "reduced-planner",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consumo = pd.read_csv(os.path.join(dir_in, file1_in), sep = ';')\n",
    "df_consumo.columns = [columna.lower() for columna in df_consumo.columns]\n",
    "df_consumo.rename(columns = {'fecha_inicio': 'mes_inicio_temp', 'fecha_fin': 'mes_fin_temp'}, inplace = True)\n",
    "df_consumo.date = pd.to_datetime(df_consumo.date, format = '%Y-%m-%d')\n",
    "precio_model = df_consumo[['ccaa', 'producto', 'volumen_miles_de_kg', 'valor_miles_de_€', 'precio_medio_kg', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "swiss-dallas",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_key_dict = {producto: precio_model[precio_model.producto == producto] for producto in precio_model.producto.unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dutch-jordan",
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba = product_key_dict['Patatas']\n",
    "train = prueba[prueba.date < '2020-01-01']\n",
    "val = prueba[(prueba.date >= '2020-01-01') & (prueba.date <'2020-03-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bored-scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_var = train[['precio_medio_kg', 'date', 'ccaa']]\n",
    "train_var.index = pd.DatetimeIndex(train_var.date)\n",
    "train_var.drop('date', axis = 1, inplace = True)\n",
    "train_var.sort_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-travel",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = SARIMAX(train, \n",
    "            order = param,\n",
    "            seasonal_order = seasonal_param, \n",
    "            enforce_stationarity = False,\n",
    "            enforce_invertibility = False)                \n",
    "result = mod.fit()\n",
    "ypred = result.get_forecast(steps = len(test)).predicted_mean.values\n",
    "error = rmse(test.values, ypred)\n",
    "error_dict.update({error: [param, seasonal_param]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "under-maple",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_var_spread = pd.pivot_table(train_var, index = train_var.index, columns = 'ccaa', values = 'precio_medio_kg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "satisfied-christianity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "maxlag=6\n",
    "test = 'ssr_chi2test'\n",
    "def grangers_causation_matrix(data, variables, test='ssr_chi2test', verbose=False):    \n",
    "    \"\"\"Check Granger Causality of all possible combinations of the Time series.\n",
    "    The rows are the response variable, columns are predictors. The values in the table \n",
    "    are the P-Values. P-Values lesser than the significance level (0.05), implies \n",
    "    the Null Hypothesis that the coefficients of the corresponding past values is \n",
    "    zero, that is, the X does not cause Y can be rejected.\n",
    "\n",
    "    data      : pandas dataframe containing the time series variables\n",
    "    variables : list containing names of the time series variables.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "    for c in df.columns:\n",
    "        for r in df.index:\n",
    "            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n",
    "            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
    "            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
    "            min_p_value = np.min(p_values)\n",
    "            df.loc[r, c] = min_p_value\n",
    "    df.columns = [var + '_x' for var in variables]\n",
    "    df.index = [var + '_y' for var in variables]\n",
    "    return df\n",
    "\n",
    "grangers_causation_matrix(train_var_spread, variables = train_var_spread.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "manual-version",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Matrix is not positive definite",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-2b537f7eec29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madjust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m':: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madjust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\">\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madjust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcvt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' =>  '\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mtrace\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mcvt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mcointegration_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_var_spread\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-44-2b537f7eec29>\u001b[0m in \u001b[0;36mcointegration_test\u001b[1;34m(df, alpha)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcointegration_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;34m\"\"\"Perform Johanson's Cointegration Test and Report Summary\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcoint_johansen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'0.90'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'0.95'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'0.99'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtraces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\vector_ar\\vecm.py\u001b[0m in \u001b[0;36mcoint_johansen\u001b[1;34m(endog, det_order, k_ar_diff)\u001b[0m\n\u001b[0;32m    626\u001b[0m     \u001b[0mau\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# au is eval, du is evec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 628\u001b[1;33m     \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mskk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    629\u001b[0m     \u001b[0mdt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mcholesky\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36mcholesky\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    762\u001b[0m     \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_commonType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m     \u001b[0msignature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D->D'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'd->d'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    765\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_nonposdef\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Matrix is not positive definite\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_raise_linalgerror_eigenvalues_nonconvergence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Matrix is not positive definite"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "\n",
    "def cointegration_test(df, alpha=0.05): \n",
    "    \"\"\"Perform Johanson's Cointegration Test and Report Summary\"\"\"\n",
    "    out = coint_johansen(df,-1,5)\n",
    "    d = {'0.90':0, '0.95':1, '0.99':2}\n",
    "    traces = out.lr1\n",
    "    cvts = out.cvt[:, d[str(1-alpha)]]\n",
    "    def adjust(val, length= 6): return str(val).ljust(length)\n",
    "\n",
    "    # Summary\n",
    "    print('Name   ::  Test Stat > C(95%)    =>   Signif  \\n', '--'*20)\n",
    "    for col, trace, cvt in zip(df.columns, traces, cvts):\n",
    "        print(adjust(col), ':: ', adjust(round(trace,2), 9), \">\", adjust(cvt, 8), ' =>  ' , trace > cvt)\n",
    "\n",
    "cointegration_test(train_var_spread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "occupational-throw",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adfuller_test(series, signif=0.05, name='', verbose=False):\n",
    "    \"\"\"Perform ADFuller to test for Stationarity of given series and print report\"\"\"\n",
    "    r = adfuller(series, autolag='AIC')\n",
    "    output = {'test_statistic':round(r[0], 4), 'pvalue':round(r[1], 4), 'n_lags':round(r[2], 4), 'n_obs':r[3]}\n",
    "    p_value = output['pvalue'] \n",
    "    def adjust(val, length= 6): return str(val).ljust(length)\n",
    "\n",
    "    # Print Summary\n",
    "    print(f'    Augmented Dickey-Fuller Test on \"{name}\"', \"\\n   \", '-'*47)\n",
    "    print(f' Null Hypothesis: Data has unit root. Non-Stationary.')\n",
    "    print(f' Significance Level    = {signif}')\n",
    "    print(f' Test Statistic        = {output[\"test_statistic\"]}')\n",
    "    print(f' No. Lags Chosen       = {output[\"n_lags\"]}')\n",
    "\n",
    "    for key,val in r[4].items():\n",
    "        print(f' Critical value {adjust(key)} = {round(val, 3)}')\n",
    "\n",
    "    if p_value <= signif:\n",
    "        print(f\" => P-Value = {p_value}. Rejecting Null Hypothesis.\")\n",
    "        print(f\" => Series is Stationary.\")\n",
    "    else:\n",
    "        print(f\" => P-Value = {p_value}. Weak evidence to reject the Null Hypothesis.\")\n",
    "        print(f\" => Series is Non-Stationary.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "decimal-opening",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "studied-longer",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Augmented Dickey-Fuller Test on \"Andalucia\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -2.1045\n",
      " No. Lags Chosen       = 0\n",
      " Critical value 1%     = -3.753\n",
      " Critical value 5%     = -2.998\n",
      " Critical value 10%    = -2.639\n",
      " => P-Value = 0.2427. Weak evidence to reject the Null Hypothesis.\n",
      " => Series is Non-Stationary.\n",
      "\n",
      "\n",
      "    Augmented Dickey-Fuller Test on \"Aragon\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -1.6211\n",
      " No. Lags Chosen       = 1\n",
      " Critical value 1%     = -3.77\n",
      " Critical value 5%     = -3.005\n",
      " Critical value 10%    = -2.643\n",
      " => P-Value = 0.4722. Weak evidence to reject the Null Hypothesis.\n",
      " => Series is Non-Stationary.\n",
      "\n",
      "\n",
      "    Augmented Dickey-Fuller Test on \"Asturias\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -2.3587\n",
      " No. Lags Chosen       = 0\n",
      " Critical value 1%     = -3.753\n",
      " Critical value 5%     = -2.998\n",
      " Critical value 10%    = -2.639\n",
      " => P-Value = 0.1537. Weak evidence to reject the Null Hypothesis.\n",
      " => Series is Non-Stationary.\n",
      "\n",
      "\n",
      "    Augmented Dickey-Fuller Test on \"Baleares\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -1.6178\n",
      " No. Lags Chosen       = 0\n",
      " Critical value 1%     = -3.753\n",
      " Critical value 5%     = -2.998\n",
      " Critical value 10%    = -2.639\n",
      " => P-Value = 0.4739. Weak evidence to reject the Null Hypothesis.\n",
      " => Series is Non-Stationary.\n",
      "\n",
      "\n",
      "    Augmented Dickey-Fuller Test on \"Canarias\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -3.5297\n",
      " No. Lags Chosen       = 0\n",
      " Critical value 1%     = -3.753\n",
      " Critical value 5%     = -2.998\n",
      " Critical value 10%    = -2.639\n",
      " => P-Value = 0.0073. Rejecting Null Hypothesis.\n",
      " => Series is Stationary.\n",
      "\n",
      "\n",
      "    Augmented Dickey-Fuller Test on \"Cantabria\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -1.7417\n",
      " No. Lags Chosen       = 0\n",
      " Critical value 1%     = -3.753\n",
      " Critical value 5%     = -2.998\n",
      " Critical value 10%    = -2.639\n",
      " => P-Value = 0.4098. Weak evidence to reject the Null Hypothesis.\n",
      " => Series is Non-Stationary.\n",
      "\n",
      "\n",
      "    Augmented Dickey-Fuller Test on \"Castilla la mancha\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -2.1921\n",
      " No. Lags Chosen       = 1\n",
      " Critical value 1%     = -3.77\n",
      " Critical value 5%     = -3.005\n",
      " Critical value 10%    = -2.643\n",
      " => P-Value = 0.2091. Weak evidence to reject the Null Hypothesis.\n",
      " => Series is Non-Stationary.\n",
      "\n",
      "\n",
      "    Augmented Dickey-Fuller Test on \"Castilla leon\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -2.3387\n",
      " No. Lags Chosen       = 0\n",
      " Critical value 1%     = -3.753\n",
      " Critical value 5%     = -2.998\n",
      " Critical value 10%    = -2.639\n",
      " => P-Value = 0.1598. Weak evidence to reject the Null Hypothesis.\n",
      " => Series is Non-Stationary.\n",
      "\n",
      "\n",
      "    Augmented Dickey-Fuller Test on \"Cataluña\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -2.0707\n",
      " No. Lags Chosen       = 1\n",
      " Critical value 1%     = -3.77\n",
      " Critical value 5%     = -3.005\n",
      " Critical value 10%    = -2.643\n",
      " => P-Value = 0.2565. Weak evidence to reject the Null Hypothesis.\n",
      " => Series is Non-Stationary.\n",
      "\n",
      "\n",
      "    Augmented Dickey-Fuller Test on \"Extremadura\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -1.3676\n",
      " No. Lags Chosen       = 2\n",
      " Critical value 1%     = -3.788\n",
      " Critical value 5%     = -3.013\n",
      " Critical value 10%    = -2.646\n",
      " => P-Value = 0.5977. Weak evidence to reject the Null Hypothesis.\n",
      " => Series is Non-Stationary.\n",
      "\n",
      "\n",
      "    Augmented Dickey-Fuller Test on \"Galicia\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -3.5326\n",
      " No. Lags Chosen       = 0\n",
      " Critical value 1%     = -3.753\n",
      " Critical value 5%     = -2.998\n",
      " Critical value 10%    = -2.639\n",
      " => P-Value = 0.0072. Rejecting Null Hypothesis.\n",
      " => Series is Stationary.\n",
      "\n",
      "\n",
      "    Augmented Dickey-Fuller Test on \"La rioja\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -1.3341\n",
      " No. Lags Chosen       = 8\n",
      " Critical value 1%     = -3.964\n",
      " Critical value 5%     = -3.085\n",
      " Critical value 10%    = -2.682\n",
      " => P-Value = 0.6134. Weak evidence to reject the Null Hypothesis.\n",
      " => Series is Non-Stationary.\n",
      "\n",
      "\n",
      "    Augmented Dickey-Fuller Test on \"Madrid\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -1.5536\n",
      " No. Lags Chosen       = 1\n",
      " Critical value 1%     = -3.77\n",
      " Critical value 5%     = -3.005\n",
      " Critical value 10%    = -2.643\n",
      " => P-Value = 0.5069. Weak evidence to reject the Null Hypothesis.\n",
      " => Series is Non-Stationary.\n",
      "\n",
      "\n",
      "    Augmented Dickey-Fuller Test on \"Murcia\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -0.4307\n",
      " No. Lags Chosen       = 8\n",
      " Critical value 1%     = -3.964\n",
      " Critical value 5%     = -3.085\n",
      " Critical value 10%    = -2.682\n",
      " => P-Value = 0.9049. Weak evidence to reject the Null Hypothesis.\n",
      " => Series is Non-Stationary.\n",
      "\n",
      "\n",
      "    Augmented Dickey-Fuller Test on \"Nacional\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -1.9032\n",
      " No. Lags Chosen       = 0\n",
      " Critical value 1%     = -3.753\n",
      " Critical value 5%     = -2.998\n",
      " Critical value 10%    = -2.639\n",
      " => P-Value = 0.3306. Weak evidence to reject the Null Hypothesis.\n",
      " => Series is Non-Stationary.\n",
      "\n",
      "\n",
      "    Augmented Dickey-Fuller Test on \"Navarra\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -2.3939\n",
      " No. Lags Chosen       = 0\n",
      " Critical value 1%     = -3.753\n",
      " Critical value 5%     = -2.998\n",
      " Critical value 10%    = -2.639\n",
      " => P-Value = 0.1435. Weak evidence to reject the Null Hypothesis.\n",
      " => Series is Non-Stationary.\n",
      "\n",
      "\n",
      "    Augmented Dickey-Fuller Test on \"Pais vasco\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -2.1829\n",
      " No. Lags Chosen       = 0\n",
      " Critical value 1%     = -3.753\n",
      " Critical value 5%     = -2.998\n",
      " Critical value 10%    = -2.639\n",
      " => P-Value = 0.2125. Weak evidence to reject the Null Hypothesis.\n",
      " => Series is Non-Stationary.\n",
      "\n",
      "\n",
      "    Augmented Dickey-Fuller Test on \"Valencia\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -2.3795\n",
      " No. Lags Chosen       = 1\n",
      " Critical value 1%     = -3.77\n",
      " Critical value 5%     = -3.005\n",
      " Critical value 10%    = -2.643\n",
      " => P-Value = 0.1476. Weak evidence to reject the Null Hypothesis.\n",
      " => Series is Non-Stationary.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, column in train_var_spread.iteritems():\n",
    "    adfuller_test(column, name=column.name)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "curious-socket",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Augmented Dickey-Fuller Test on \"Andalucia\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -2.1332\n",
      " No. Lags Chosen       = 9\n",
      " Critical value 1%     = -4.069\n",
      " Critical value 5%     = -3.127\n",
      " Critical value 10%    = -2.702\n",
      " => P-Value = 0.2314. Weak evidence to reject the Null Hypothesis.\n",
      " => Series is Non-Stationary.\n",
      "\n",
      "\n",
      "    Augmented Dickey-Fuller Test on \"Aragon\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -7.641\n",
      " No. Lags Chosen       = 0\n",
      " Critical value 1%     = -3.77\n",
      " Critical value 5%     = -3.005\n",
      " Critical value 10%    = -2.643\n",
      " => P-Value = 0.0. Rejecting Null Hypothesis.\n",
      " => Series is Stationary.\n",
      "\n",
      "\n",
      "    Augmented Dickey-Fuller Test on \"Asturias\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -3.9539\n",
      " No. Lags Chosen       = 9\n",
      " Critical value 1%     = -4.069\n",
      " Critical value 5%     = -3.127\n",
      " Critical value 10%    = -2.702\n",
      " => P-Value = 0.0017. Rejecting Null Hypothesis.\n",
      " => Series is Stationary.\n",
      "\n",
      "\n",
      "    Augmented Dickey-Fuller Test on \"Baleares\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -4.211\n",
      " No. Lags Chosen       = 3\n",
      " Critical value 1%     = -3.833\n",
      " Critical value 5%     = -3.031\n",
      " Critical value 10%    = -2.656\n",
      " => P-Value = 0.0006. Rejecting Null Hypothesis.\n",
      " => Series is Stationary.\n",
      "\n",
      "\n",
      "    Augmented Dickey-Fuller Test on \"Canarias\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = 0.5427\n",
      " No. Lags Chosen       = 9\n",
      " Critical value 1%     = -4.069\n",
      " Critical value 5%     = -3.127\n",
      " Critical value 10%    = -2.702\n",
      " => P-Value = 0.9861. Weak evidence to reject the Null Hypothesis.\n",
      " => Series is Non-Stationary.\n",
      "\n",
      "\n",
      "    Augmented Dickey-Fuller Test on \"Cantabria\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -2.14\n",
      " No. Lags Chosen       = 9\n",
      " Critical value 1%     = -4.069\n",
      " Critical value 5%     = -3.127\n",
      " Critical value 10%    = -2.702\n",
      " => P-Value = 0.2288. Weak evidence to reject the Null Hypothesis.\n",
      " => Series is Non-Stationary.\n",
      "\n",
      "\n",
      "    Augmented Dickey-Fuller Test on \"Castilla la mancha\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -8.218\n",
      " No. Lags Chosen       = 0\n",
      " Critical value 1%     = -3.77\n",
      " Critical value 5%     = -3.005\n",
      " Critical value 10%    = -2.643\n",
      " => P-Value = 0.0. Rejecting Null Hypothesis.\n",
      " => Series is Stationary.\n",
      "\n",
      "\n",
      "    Augmented Dickey-Fuller Test on \"Castilla leon\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -5.6111\n",
      " No. Lags Chosen       = 0\n",
      " Critical value 1%     = -3.77\n",
      " Critical value 5%     = -3.005\n",
      " Critical value 10%    = -2.643\n",
      " => P-Value = 0.0. Rejecting Null Hypothesis.\n",
      " => Series is Stationary.\n",
      "\n",
      "\n",
      "    Augmented Dickey-Fuller Test on \"Cataluña\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -5.7825\n",
      " No. Lags Chosen       = 0\n",
      " Critical value 1%     = -3.77\n",
      " Critical value 5%     = -3.005\n",
      " Critical value 10%    = -2.643\n",
      " => P-Value = 0.0. Rejecting Null Hypothesis.\n",
      " => Series is Stationary.\n",
      "\n",
      "\n",
      "    Augmented Dickey-Fuller Test on \"Extremadura\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -5.175\n",
      " No. Lags Chosen       = 1\n",
      " Critical value 1%     = -3.788\n",
      " Critical value 5%     = -3.013\n",
      " Critical value 10%    = -2.646\n",
      " => P-Value = 0.0. Rejecting Null Hypothesis.\n",
      " => Series is Stationary.\n",
      "\n",
      "\n",
      "    Augmented Dickey-Fuller Test on \"Galicia\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -6.1399\n",
      " No. Lags Chosen       = 1\n",
      " Critical value 1%     = -3.788\n",
      " Critical value 5%     = -3.013\n",
      " Critical value 10%    = -2.646\n",
      " => P-Value = 0.0. Rejecting Null Hypothesis.\n",
      " => Series is Stationary.\n",
      "\n",
      "\n",
      "    Augmented Dickey-Fuller Test on \"La rioja\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -2.6132\n",
      " No. Lags Chosen       = 7\n",
      " Critical value 1%     = -3.964\n",
      " Critical value 5%     = -3.085\n",
      " Critical value 10%    = -2.682\n",
      " => P-Value = 0.0903. Weak evidence to reject the Null Hypothesis.\n",
      " => Series is Non-Stationary.\n",
      "\n",
      "\n",
      "    Augmented Dickey-Fuller Test on \"Madrid\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -7.1735\n",
      " No. Lags Chosen       = 0\n",
      " Critical value 1%     = -3.77\n",
      " Critical value 5%     = -3.005\n",
      " Critical value 10%    = -2.643\n",
      " => P-Value = 0.0. Rejecting Null Hypothesis.\n",
      " => Series is Stationary.\n",
      "\n",
      "\n",
      "    Augmented Dickey-Fuller Test on \"Murcia\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -2.5695\n",
      " No. Lags Chosen       = 9\n",
      " Critical value 1%     = -4.069\n",
      " Critical value 5%     = -3.127\n",
      " Critical value 10%    = -2.702\n",
      " => P-Value = 0.0995. Weak evidence to reject the Null Hypothesis.\n",
      " => Series is Non-Stationary.\n",
      "\n",
      "\n",
      "    Augmented Dickey-Fuller Test on \"Nacional\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -4.5846\n",
      " No. Lags Chosen       = 0\n",
      " Critical value 1%     = -3.77\n",
      " Critical value 5%     = -3.005\n",
      " Critical value 10%    = -2.643\n",
      " => P-Value = 0.0001. Rejecting Null Hypothesis.\n",
      " => Series is Stationary.\n",
      "\n",
      "\n",
      "    Augmented Dickey-Fuller Test on \"Navarra\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -2.0529\n",
      " No. Lags Chosen       = 9\n",
      " Critical value 1%     = -4.069\n",
      " Critical value 5%     = -3.127\n",
      " Critical value 10%    = -2.702\n",
      " => P-Value = 0.2639. Weak evidence to reject the Null Hypothesis.\n",
      " => Series is Non-Stationary.\n",
      "\n",
      "\n",
      "    Augmented Dickey-Fuller Test on \"Pais vasco\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -7.6435\n",
      " No. Lags Chosen       = 0\n",
      " Critical value 1%     = -3.77\n",
      " Critical value 5%     = -3.005\n",
      " Critical value 10%    = -2.643\n",
      " => P-Value = 0.0. Rejecting Null Hypothesis.\n",
      " => Series is Stationary.\n",
      "\n",
      "\n",
      "    Augmented Dickey-Fuller Test on \"Valencia\" \n",
      "    -----------------------------------------------\n",
      " Null Hypothesis: Data has unit root. Non-Stationary.\n",
      " Significance Level    = 0.05\n",
      " Test Statistic        = -5.0696\n",
      " No. Lags Chosen       = 0\n",
      " Critical value 1%     = -3.77\n",
      " Critical value 5%     = -3.005\n",
      " Critical value 10%    = -2.643\n",
      " => P-Value = 0.0. Rejecting Null Hypothesis.\n",
      " => Series is Stationary.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_differenced = train_var_spread.diff().dropna()\n",
    "for name, column in df_differenced.iteritems():\n",
    "    adfuller_test(column, name=column.name)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "funky-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.vector_ar.var_model import VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "breathing-instruction",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lag Order = 1\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "5-th leading minor of the array is not positive definite",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-31d28ae34a60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxlags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Lag Order ='\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'AIC : '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'BIC : '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'FPE : '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfpe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\statsmodels\\base\\wrapper.py\u001b[0m in \u001b[0;36m__getattribute__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mhow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrap_attrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\vector_ar\\var_model.py\u001b[0m in \u001b[0;36maic\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2134\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0maic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2135\u001b[0m         \u001b[1;34m\"\"\"Akaike information criterion\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2136\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo_criteria\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'aic'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2138\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\vector_ar\\var_model.py\u001b[0m in \u001b[0;36minfo_criteria\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2115\u001b[0m         \u001b[0mfree_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlag_order\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mneqs\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mneqs\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk_exog\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2117\u001b[1;33m         \u001b[0mld\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogdet_symm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigma_u_mle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2119\u001b[0m         \u001b[1;31m# See Lütkepohl pp. 146-150\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\statsmodels\\tools\\linalg.py\u001b[0m in \u001b[0;36mlogdet_symm\u001b[1;34m(m, check_symm)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# would be nice to short-circuit check\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"m is not symmetric.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcho_factor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiagonal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp_cholesky.py\u001b[0m in \u001b[0;36mcho_factor\u001b[1;34m(a, lower, overwrite_a, check_finite)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \"\"\"\n\u001b[0;32m    152\u001b[0m     c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=False,\n\u001b[1;32m--> 153\u001b[1;33m                          check_finite=check_finite)\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp_cholesky.py\u001b[0m in \u001b[0;36m_cholesky\u001b[1;34m(a, lower, overwrite_a, clean, check_finite)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n\u001b[1;32m---> 38\u001b[1;33m                           \"definite\" % info)\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         raise ValueError('LAPACK reported an illegal value in {}-th argument'\n",
      "\u001b[1;31mLinAlgError\u001b[0m: 5-th leading minor of the array is not positive definite"
     ]
    }
   ],
   "source": [
    "model = VAR(df_differenced)\n",
    "for i in [1,2,3,4,5,6,7,8,9]:\n",
    "    result = model.fit(maxlags = 1)\n",
    "    print('Lag Order =', i)\n",
    "    print('AIC : ', result.aic)\n",
    "    print('BIC : ', result.bic)\n",
    "    print('FPE : ', result.fpe)\n",
    "    print('HQIC: ', result.hqic, '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
